{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32456efd-9fa0-48ec-8954-81c1c1d1cc05",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "</br>\n",
    "The code in this notebook was adapted from langchain's simple <a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>RAG application walkthrough</a> and <a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's streamlit RAG demo</a>.  \n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decc5ef-fc5e-4297-8072-e036de9932e4",
   "metadata": {},
   "source": [
    "Before running this notebook, make sure to open the terminal and run `pip install -r requirements.txt` to load the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b4057-43d0-4390-abe6-9fe05cad22d5",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Setting up RAG\n",
    "\n",
    "This portion of the notebook will walk through the code used to set up our RAG system for the demos.  \n",
    "    </br>\n",
    "To run all the code in this section and skip to the demo, click the table of contents icon on the left menu bar. Then right click the title of this section, and choose 'Select and Run Cell(s) for this Heading'. Then click the Demos heading to skip to that portion of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692f18e-8e5f-4672-809a-ca601ecfbdfa",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "    \n",
    "## Initial Setup\n",
    "\n",
    "First we'll set up the chatbot, embedding model, and embedding storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2054b-7176-41d7-9870-d442e79fb796",
   "metadata": {},
   "source": [
    "**General Questions:**  \n",
    "What llm, embedding model, and vector store should I use? - we'll just pick one arbitrarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842805d6-3de9-4eaf-8bf2-cc5d17bf273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecffcc-b1c0-4ff9-8d56-a4a4a17733ef",
   "metadata": {},
   "source": [
    "### Ask for your OpenAI API key if you haven't already set one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81873c5-0716-4b25-8ec9-fd58f1c132e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for OpenAI:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  api_key = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    #should I also do: os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    #or do I not want to mess with their os environment\n",
    "else:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8862d-8110-4129-87f4-ef8d2e7183ea",
   "metadata": {},
   "source": [
    "**TODO:** Replace with some call to a secrets folder for easier use while developing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409255bb-a62f-4f14-84a5-a00bc4c10a6d",
   "metadata": {},
   "source": [
    "### Set up the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1668a103-3d86-404e-a3d3-90847725a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU \"langchain-openai\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7edef9a-e026-4c2b-9f3b-6a82a2594afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"llama3\", api_key = api_key, base_url = \"https://llm.nrp-nautilus.io\",  temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf329b70-6c8d-4fdd-a4fc-760e14b4374c",
   "metadata": {},
   "source": [
    "### Set up the embedding model\n",
    "[insert one setence explanation for embedding, and link to further explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc2684a-4582-47ff-8e72-c82fee4c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model = \"embed-mistral\", \n",
    "    api_key = api_key, \n",
    "    base_url = \"https://llm.nrp-nautilus.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddf1f8-efc6-452f-8e8c-40875c932ec6",
   "metadata": {},
   "source": [
    "**Perhaps change this ^ to embed mistral**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e4df9-6577-4f8b-9b2b-7b2aa9ff67e0",
   "metadata": {},
   "source": [
    "### Set up the embedding storage system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4078a267-2aec-4e8a-8ffd-93b38e23f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef1a93c-8035-42bc-82e0-bdeec2fcb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aef89-1cfd-49f5-ae99-14cf9ed7595f",
   "metadata": {},
   "source": [
    "### Initial Setup Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8559c0e-5dbe-4372-a993-c15ebdaec666",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Data Processing Pipeline (Indexing)\n",
    "Here's where we start processing the textual data in the document(s) we want our chatbot to use when answering our questions. In our case, this will involve 3 steps: \n",
    "\n",
    "1. Load the document(s)    \n",
    "2. Split the document(s) into smaller pieces  \n",
    "3. Produce vectors representing these smaller pieces, and use those vectors to organize our pieces in a database\n",
    "\n",
    "If we want to change the document(s) our chatbot is using, we'll have to add the new documents and run through this part of the process again (hence the name 'pipeline')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601655d-7436-4008-bd79-d582f5812855",
   "metadata": {},
   "source": [
    "### Load the document(s)\n",
    "This code allows us to load the textual data from PDFs into a format that we can work with. You can also load html files directly from the web by following the steps described in \n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/#loading-documents' target='_blank'>the 'loading documents' portion of the RAG application walkthrough</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68785407-43bb-43a6-b0c9-f86bfe0bf0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-community pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335bfbca-7652-44c2-b550-25bba658abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "def pdf_loader(url):\n",
    "    \"\"\"\n",
    "    Loads the PDF at the given url.\n",
    "\n",
    "    Arguments:\n",
    "        url: the url to the PDF you want to load\n",
    "\n",
    "    Returns: A document containing the text data (and metadata) of the specified PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(url)\n",
    "    return loader.load()\n",
    "\n",
    "docs = pdf_loader('https://canature.maps.arcgis.com/sharing/rest/content/items/8da9faef231c4e31b651ae6dff95254e/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972fd63-4d16-4c92-a34b-b58b4aa151b1",
   "metadata": {},
   "source": [
    "To load multiple PDFs: put all the PDFs in a folder, uncomment the last line of the cell below, paste in the path to your folder, and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b44d8aa-9989-4193-afe6-0d6d896488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_pdf_loader(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all PDFs in the specified folder.\n",
    "\n",
    "    Arguments:\n",
    "        folder_path: the path to the folder containing the all the PDFs you want to load.\n",
    "\n",
    "    Returns: A list of documents, each document representing one PDF\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    return loader.load()\n",
    "\n",
    "#Uncomment the line below and paste in the path to your pdf folder to load multiple PDFs. An example folder file path would look like: 'C:/Users/evanl/Downloads/PDF Folder Name'\n",
    "#docs = multiple_pdf_loader('paste the path to your folder here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b0c15-243c-4e9d-af79-0d797d1126fc",
   "metadata": {},
   "source": [
    "**TODO:** Test the multiple_pdf_loader function  \n",
    "Also should I remove the docstrings? Do they make the code look scarier than it is just because it makes the cell look much bigger than it should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ca3ac-214d-4902-ae21-c543e47a2cad",
   "metadata": {},
   "source": [
    "### Split the document(s) into bite-sized pieces\n",
    "This code will take our document(s) and split their text into smaller sub-sections, sometimes referred to as 'chunks'. There are two important parameters to note in the cell below: `chunk_size` and `chunk_overlap`. \n",
    "\n",
    "The `chunk_size` parameter determines (approximately) how many characters will be in each chunk. The `chunk_overlap` parameter determines how many characters will be shared by any given chunk and the chunk that directly follows it in the text. The importance of `chunk_overlap` is discussed in the article (see breaking mode 1), and will be demonstrated later in this notebook.\n",
    "\n",
    "You can read more about langchain's text splitting methods [here](https://python.langchain.com/docs/how_to/recursive_text_splitter/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "139d2b2a-d4d3-4d28-b242-7e8248857017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split pdf into 188 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split pdf into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2007b3c-7289-4a7b-8304-0a54c42e646d",
   "metadata": {},
   "source": [
    "### Add the pieces to the vector store\n",
    "Under the hood, this code is actually doing two things. When we set up the vector store earlier, we told it which embedding model to use. Now, when we the add the chunks of our documents to the vector store, it first will call the embedding model to create vector representations of those chunks. Then it will use those vector representations to organize the chunks within the database. This will allow us to  quickly search for relevant pieces of our document(s) later.\n",
    "\n",
    "**TODO:** fact check my description of the under-the-hood activities (I think it's true, but that's just because I don't see how else it could work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764fa06a-44ee-44f4-b80e-7d5256c66f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28556928-7cdd-43bd-af31-93e74241c729', '7edc9640-d0d7-42d1-9da5-127cede7823b', '091afac0-7b78-49fb-ae25-7c13a3270670']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b77f2-9cb5-4356-81df-52c71886e13b",
   "metadata": {},
   "source": [
    "### Indexing Complete!\n",
    "\n",
    "At this point we've completed the 'indexing' portion of our set up process. This has involved 3 steps:  \n",
    "\n",
    "1. **Loading our document(s)**: We used PyPDFLoader to load our pdf(s) into a format we could process using code.\n",
    "2. **Text Splitting**: We used a text splitter to break our document(s) into smaller pieces that our chatbot will be able to more easily digest.  \n",
    "3. **Add chunks to our vector storage system**: We used an embedding model to represeent the pieces of our document(s) as vectors. Utilizing the vector embeddings we just made, we organized the pieces of our document(s) in a database.\n",
    "                                                                                                                                                                                                        \n",
    "Next we will set up a 'retriever' which will use this organized database to retrieve relevant pieces of our document based on the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58198c46-7394-49bb-a95f-fe52a52d7b05",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd3c04-40db-4e52-b512-f05ac056d778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d38c6ae-f783-42b4-a140-6b8bd1a1c7d8",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf7c2-44f1-4abb-b101-2723d6436ebe",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d3d0a-2b2f-4523-9715-551adfa13559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6d4eaf-c2b3-400c-913f-5fb55ddd68df",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62282f3a-d7de-401a-ab6e-d2cf3076221a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5067f5-dd80-4f2a-9771-6b884145cf19",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style='border: 3px solid #0D335F;' />\n",
    "<hr style='border: 1px solid #5FAE5B;' />\n",
    "\n",
    "# Sources\n",
    "This is a collection of all the links I inserted throughout the doc\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/rag/  \n",
    "https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py  \n",
    "https://python.langchain.com/docs/how_to/recursive_text_splitter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71728bb-f7bf-4428-a564-1e9ed4e2abd0",
   "metadata": {},
   "source": [
    "### Dump:\n",
    "\n",
    "**Breaking mode 1:**  \n",
    "Higher chunk overlap increases the chance that, if one chunk is deemed relevant to the prompt, the chunks surrounding it will also be seen as relevant. In effect, this encourages the RAG model to read more of the context surrounding the chunk where it believes an answer is located. The downside of high chunk overlap is increased computational intensity, since higher overlap means there will be more chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9fff4-52d5-48f4-8c58-5ea6055e66b4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "How should we set up the notebook so users can conveniently enter their OpenAI API key?\n",
    "\n",
    "What do we think about the blue and green horizontal lines? Are there tweaks we could make that would be better?\n",
    "\n",
    "I assume the actual notebook shouldn't have the %pip install cells right?  \n",
    "And how do I add the -U in requirements.txt (or I guess just the U since I assume the q just means don't fill the screen with text)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b80262-c460-43bd-bf24-d0f7ef20b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
