{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32456efd-9fa0-48ec-8954-81c1c1d1cc05",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "</br>\n",
    "The code in this notebook was adapted from langchain's simple <a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>RAG application walkthrough</a> and <a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's streamlit RAG demo</a>.  \n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decc5ef-fc5e-4297-8072-e036de9932e4",
   "metadata": {},
   "source": [
    "Before running this notebook, make sure to open the terminal and run `pip install -r requirements.txt` to load the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b4057-43d0-4390-abe6-9fe05cad22d5",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Setting up RAG\n",
    "\n",
    "This portion of the notebook will walk through the code used to set up our RAG system for the demos.\n",
    "\n",
    "To run all the code in this section and skip to the demo, click the table of contents icon on the left menu bar. Then right click the title of this section, and choose 'Select and Run Cell(s) for this Heading'. Then click the Demos heading to skip to that portion of the notebook. Note that it may take a minute for all the setup cells to finish running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692f18e-8e5f-4672-809a-ca601ecfbdfa",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "    \n",
    "## Initial Setup\n",
    "\n",
    "First we'll set up the chatbot and embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecffcc-b1c0-4ff9-8d56-a4a4a17733ef",
   "metadata": {},
   "source": [
    "### Ask for your OpenAI API key if you haven't already set one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c81873c5-0716-4b25-8ec9-fd58f1c132e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    api_key = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    #TODO: Remove this ^ to avoid messing with their os.environ? Or is it ok?\n",
    "else:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8862d-8110-4129-87f4-ef8d2e7183ea",
   "metadata": {},
   "source": [
    "**TODO:** Replace with some call to a secrets folder for easier use while developing? Make sure this code is safe to distribute (i.e. make sure the fact that me having run the code doesn't mean someone else can come in this notebook and access my API key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409255bb-a62f-4f14-84a5-a00bc4c10a6d",
   "metadata": {},
   "source": [
    "### Set up the chatbot's language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a7edef9a-e026-4c2b-9f3b-6a82a2594afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"llama3\", api_key = api_key, base_url = \"https://llm.nrp-nautilus.io\",  temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf329b70-6c8d-4fdd-a4fc-760e14b4374c",
   "metadata": {},
   "source": [
    "### Set up the embedding model\n",
    "An embedding model is a machine that, in this case, can take textual data and produce a vector representation (an embedding) of that piece of text. These vector representations allow us to quickly identify sematically similar pieces of text using linear algebra. To read more about embeddings, check out <a href='https://www.ibm.com/think/topics/embedding' target='_blank'>this article from IMB.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6bc2684a-4582-47ff-8e72-c82fee4c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model = \"embed-mistral\", \n",
    "    api_key = api_key, \n",
    "    base_url = \"https://llm.nrp-nautilus.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aef89-1cfd-49f5-ae99-14cf9ed7595f",
   "metadata": {},
   "source": [
    "### Initial Setup Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8559c0e-5dbe-4372-a993-c15ebdaec666",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Data Processing Pipeline (Indexing)\n",
    "Here's where we start processing the textual data in the document(s) we want our chatbot to use when answering our questions. In our case, this will involve 3 steps: \n",
    "\n",
    "1. Load the document(s)    \n",
    "2. Split the document(s) into smaller pieces  \n",
    "3. Produce vectors representing these smaller pieces, and use those vectors to organize our pieces in a database\n",
    "\n",
    "If we want to change the document(s) our chatbot is using, we'll have to add the new documents and run through this part of the process again (hence the name 'pipeline')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601655d-7436-4008-bd79-d582f5812855",
   "metadata": {},
   "source": [
    "### Load the document(s)\n",
    "This code allows us to load the textual data from PDFs into a format that we can work with. You can also load html files directly from the web by following the steps described in \n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/#loading-documents' target='_blank'>the 'loading documents' portion of the RAG application walkthrough</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "335bfbca-7652-44c2-b550-25bba658abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "def pdf_loader(url):\n",
    "    \"\"\"\n",
    "    Loads the PDF at the given url.\n",
    "\n",
    "    Args:\n",
    "        url (str): the url to the PDF you want to load\n",
    "\n",
    "    Returns: A document containing the text data (and metadata) of the specified PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(url)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e3e3b634-4c18-41a8-aced-5f1a38bdc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pdf_loader('https://canature.maps.arcgis.com/sharing/rest/content/items/8da9faef231c4e31b651ae6dff95254e/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972fd63-4d16-4c92-a34b-b58b4aa151b1",
   "metadata": {},
   "source": [
    "To load multiple PDFs: put all the PDFs in a folder, add the PDF folder to the folder containing this jupyter notebook (or any other location were it can be accessed by this jupyter notebook), uncomment the last line of the cell below, write in the path to your folder, and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7b44d8aa-9989-4193-afe6-0d6d896488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_pdf_loader(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all PDFs in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): path to the folder containing the all the PDFs you want to load.\n",
    "\n",
    "    Returns: A list of documents, each document representing one PDF\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    return loader.load()\n",
    "\n",
    "#If the PDF folder is in the same folder as this jupyter notebook, the folder path is just the PDF folder's name\n",
    "#Uncomment the line below and paste in the path to your pdf folder to load multiple PDFs.\n",
    "#docs = multiple_pdf_loader('PDF Folder Path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ca3ac-214d-4902-ae21-c543e47a2cad",
   "metadata": {},
   "source": [
    "### Split the document(s) into bite-sized pieces\n",
    "This code will take our document(s) and split their text into smaller sub-sections, sometimes referred to as 'chunks'. There are two important parameters to note in the cell below: `chunk_size` and `chunk_overlap`. \n",
    "\n",
    "The `chunk_size` parameter determines (approximately) how many characters will be in each chunk. The `chunk_overlap` parameter determines how many characters will be shared by any given chunk and the chunk that directly follows it in the text. The importance of `chunk_overlap` is discussed in the article (see breaking mode 1), and will be demonstrated later in this notebook.\n",
    "\n",
    "You can read more about langchain's text splitting methods <a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "139d2b2a-d4d3-4d28-b242-7e8248857017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split pdf into 188 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split pdf into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2007b3c-7289-4a7b-8304-0a54c42e646d",
   "metadata": {},
   "source": [
    "### Make an embedding storage system and add the chunks to this storage system\n",
    "First, we'll initialize an embedding storage system, sometimes referred to as a vector store, that will use the embedding model we set up earlier (`embeddings`). Then, when we the add the chunks of our documents to the vector store, it will call the embedding model to create vector representations of those chunks. The vector store will use those vector representations to organize the chunks within its database. This will allow us to  quickly search for relevant pieces of our document(s) later.\n",
    "\n",
    "**TODO:** fact check my description of the under-the-hood activities (I think it's true, but that's just because I don't see how else it could work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "764fa06a-44ee-44f4-b80e-7d5256c66f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dd41d987-265e-4f97-a4b1-df4f3f09ccc4', '78a4bfb2-fb9f-48b9-b36a-005207bcdf02', '8d8adb6a-6071-4909-ab93-0a6c6716412a']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b77f2-9cb5-4356-81df-52c71886e13b",
   "metadata": {},
   "source": [
    "### Indexing Complete!\n",
    "\n",
    "At this point we've completed the 'indexing' portion of our set up process. This has involved 3 steps:  \n",
    "\n",
    "1. **Loading our document(s)**: We used PyPDFLoader to load our pdf(s) into a format we could process using code.\n",
    "2. **Text Splitting**: We used a text splitter to break our document(s) into smaller pieces that our LLM will be able to more easily digest.  \n",
    "3. **Add chunks to our vector storage system**: We used an embedding model to represeent the pieces of our document(s) as vectors. Utilizing the vector embeddings we just made, we organized the pieces of our document(s) in a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58198c46-7394-49bb-a95f-fe52a52d7b05",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Retrieval and Generation\n",
    "\n",
    "Next we will build the infrastructure to find relevant pieces of our document(s) based on our query, and then pass our query along with those relevant pieces of text to the LLM so it can generate an informed response. \n",
    "\n",
    "Retrieving relevant chunks of our document(s) based on our question is often referred to as 'retrieval'.\n",
    "\n",
    "Note: The instructions in the Retrieval and Generation portion of LangChain's RAG demo, <a href='https://python.langchain.com/docs/tutorials/rag/#orchestration' target='_blank'>linked here</a>, uses code more conducive to future modifications and integrations into larger systems. This flexible code is likely preferable when developing a real RAG application, but is more complex than necessary for demonstration purposes. In this demo, we'll take a more 'quick and dirty' approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fbe38-6815-485e-9475-5f324c0dccea",
   "metadata": {},
   "source": [
    "### Make a template for the prompt we'll pass to our LLM\n",
    "\n",
    "With a template we can pass in our question and the relevant context, and we'll get back one complete prompt to pass to our LLM. We'll do this using LangChain's ChatPromptTemplate (<a href='https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html' target='_blank'> documentation linked here</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "eb66bf59-8435-4e55-b47c-2baf03b30042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Context: [I'll put the context here!]\n",
      "Question: [I'll put the user's question here!]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt_template = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = prompt.invoke(\n",
    "    {\"context\": \"[I'll put the context here!]\", \"input\": \"[I'll put the user's question here!]\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_prompt[0].content)\n",
    "print(example_prompt[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb5676-204e-4a16-9cc2-e363a845394b",
   "metadata": {},
   "source": [
    "### Build a way for the user to query the RAG Chatbot\n",
    "\n",
    "Each time you query a RAG Chatbot, 3 things happen:\n",
    "\n",
    "1. The vector store finds the chunks of the document most relevant to your question (this is the 'retrieval' step).\n",
    "2. Your question and the relevant chunks are bundled into one big prompt.\n",
    "3. That prompt is passed to the LLM, and the LLM uses the relevant chunks to answer your question (this is the 'generation' step).\n",
    "\n",
    "So, we'll build a function that does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c041991d-47b2-4f9c-90a4-d2f4851b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_RAG(question: str):\n",
    "    relevant_chunks = vector_store.similarity_search(question) \n",
    "    #searches the vector store for chunks semantically similar to the question \n",
    "    #these chunks are returned as LangChain Document objects\n",
    "    \n",
    "    context_str = '\\n\\n'.join(chunk.page_content for chunk in relevant_chunks) \n",
    "    #chunk.page_content gets the chunk's text (since each chunk is a Document object)\n",
    "    #'/n/n'.join(...) builds a string with two new lines between each relevant chunk\n",
    "    \n",
    "    prompt_with_context = prompt.invoke(\n",
    "        {'context': context_str, 'input': question}\n",
    "    )\n",
    "    #builds a prompt using the context string and user's question\n",
    "    \n",
    "    response = llm.invoke(prompt_with_context)\n",
    "    #gives the prompt to the LLM and gets the LLM's response\n",
    "    \n",
    "    return {'answer': response.content, 'context': context_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "5a23af72-5ad4-4f52-83a3-0e9c7b7c3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to achieve 30x30 are: \n",
      "\n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "pathway_response = ask_RAG(\"What are the 10 pathways to 30x30\")\n",
    "print(pathway_response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38c6ae-f783-42b4-a140-6b8bd1a1c7d8",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf7c2-44f1-4abb-b101-2723d6436ebe",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 1: Chunk Cutoffs\n",
    "\n",
    "Because the document is broken into chunks, and the LLM is only provided the chunks that the vector store thinks are most similar to the user's question, chunks neighboring each other in the text may be seperated. One could be deemed relevant and passed to the LLM while the other is deemed not sufficiently relevant and left out. Low chunk_size and chunk_overlap increase the risk of this being a problem, as shown in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd799f3-eec6-4445-9ca2-f5abe9fb18ae",
   "metadata": {},
   "source": [
    "### RAG Builder Set Up\n",
    "\n",
    "This is a helper function for the demos. The function takes a vector store and builds an ask_RAG function using that vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "5ed3d99a-78fe-4d44-aa42-6220fb8b13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ask_RAG(vector_store):\n",
    "    \"\"\"\n",
    "    Build a RAG Chatbot that uses the given vector store.\n",
    "\n",
    "    Args: \n",
    "        vector_store: an instance of Langchian's InMemoryVectorStore\n",
    "\n",
    "    Returns: an ask_RAG function built on that vector store\n",
    "    \"\"\"\n",
    "    def new_ask_RAG(question: str):\n",
    "        relevant_chunks = vector_store.similarity_search(question) \n",
    "        context_str = '\\n\\n'.join(chunk.page_content for chunk in relevant_chunks) \n",
    "        prompt_with_context = prompt.invoke(\n",
    "            {'context': context_str, 'input': question}\n",
    "        )\n",
    "        response = llm.invoke(prompt_with_context)\n",
    "        return {'answer': response.content, 'context': context_str}\n",
    "    \n",
    "    return new_ask_RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0cc5f-996b-4742-a81f-8431e111f671",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Myopic RAG Agent\n",
    "\n",
    "We'll create a RAG Agent based on a document that we split up into small chunks with small chunk_overlap. In this case, `chunk_size = 500` and `chunk_overlap = 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "ad472f9c-acfd-43dd-849c-9e880a077bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_30x30_text = pdf_loader('https://canature.maps.arcgis.com/sharing/rest/content/items/8da9faef231c4e31b651ae6dff95254e/data')\n",
    "# This is the same document that was used in the set-up portion of this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3a9d3d0a-2b2f-4523-9715-551adfa13559",
   "metadata": {},
   "outputs": [],
   "source": [
    "myopic_chunk_size = 500\n",
    "myopic_chunk_overlap = 100\n",
    "\n",
    "myo_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = myopic_chunk_size,\n",
    "    chunk_overlap = myopic_chunk_overlap,\n",
    ")\n",
    "myo_splits = myo_text_splitter.split_documents(long_30x30_text)\n",
    "\n",
    "myo_vector_store = InMemoryVectorStore(embeddings)\n",
    "myo_vector_store.add_documents(documents=myo_splits)\n",
    "\n",
    "myo_ask_RAG = build_ask_RAG(myo_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "51ca7b89-f9f7-4cb6-b41d-c0bfea1e364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not list all 10 pathways to 30x30. However, it mentions three pathways: \n",
      "\n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "\n",
      "Additionally, it mentions two more pathways (10.11 and 10.12) but does not provide the complete list of 10 pathways.\n"
     ]
    }
   ],
   "source": [
    "myo_results = myo_ask_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(myo_results['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38853ae6-bfa6-4367-80c5-3490cd498e8c",
   "metadata": {},
   "source": [
    "### Myopic Results\n",
    "\n",
    "Clearly the myopic RAG model, with low chunk size and chunk overlap, produced an incomplete answer. Fortunately, the model admitted that it didn't know the remaining 7 pathways. Other models we've tested have not been as forthcoming. Some hallucinated the pathways they couldn't find.\n",
    "To see the context this model was given to answer this question, change the value of `show_context_myo` to `True` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a99636ee-e2fd-4b2b-9e51-57b87fb29244",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_myo = False #Change this to True to see the context the myopic LLM was given to answer this question\n",
    "\n",
    "if show_context_myo:\n",
    "    print(myo_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3c346-c5aa-43e6-9ff1-88e52304f6b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Baseline RAG Agent for Comparison\n",
    "\n",
    "The RAG Agent we built in the set up portion of this notebook had a `chunk_size` of 1000 and a `chunk_overlap` of 200. These are the values used in <a href='https://python.langchain.com/docs/tutorials/rag/#preview' target='_blank'>LangChain's RAG application walkthrough</a>. We'll ask it the same question to see how it fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e6fdfd8c-ad06-41af-abd5-76594e1a680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to achieve 30x30 are: \n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "baseline_results = ask_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(baseline_results['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d7324-92c7-40ae-9c43-0f50af251a04",
   "metadata": {},
   "source": [
    "### Baseline Results\n",
    "\n",
    "The RAG agent was provided enough context to correctly identify all 10 pathways. To see the context this model was given, change the value of `show_context_baseline` to `True` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "12f4475c-c49f-46ed-abd3-52480236a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_baseline = False #Change this to True to see the context the baselinen LLM was given to answer this question\n",
    "\n",
    "if show_context_baseline:\n",
    "    print(baseline_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857c19e-4b2d-4dca-873e-350033d23384",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**TODO:** Rehash some of the interpretation? Maybe just a couple sentence summary of what was said in the paper for a refresher?\n",
    "\n",
    "**Note:** Values of `chunk_size` and `chunk_overlap` at which we observe incorrect answers may change with new versions, but qualitatively the observed patterns are likely to persist.  \n",
    "As new versions of the packages in this demo have released, the values of `chunk_size` and `chunk_overlap` at which the RAG Chatbot produced incorrect answers changed. Typically, when using the more recent versions of these packages, the model was able to accurately answer the prompt for lower values of `chunk_size` and `chunk_overlap`. Increases in the efficiency of tokenization, allowing each chunk to contain more text using fewer tokens, could account for this change. Qualitatively, though, the patterns we observed remained the same. Lower values of `chunk_size` and `chunk_overlap` increased the risk of incomplete answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b7603-e173-4d40-8413-3e2fc8302bd0",
   "metadata": {},
   "source": [
    "### Potential Solutions\n",
    "\n",
    "**Increasing chunk size** forces more neighboring text to be kept together because higher chunk size produces larger units of text. However, these large chunks may still be seperated from their neighbors, and cuts between chunks may still appear in inconvenient places. Increasing chunk size reduces the total number of cuts, but does not address the underlying problem that chunks may be seperated from their neighbors.\n",
    "\n",
    "**Increasing chunk overlap** may be a more comprehensive solution. Higher chunk overlap increases the amount of text shared by two neighboring chunks, thereby increasing their similarity. Higher similarity amongst neighboring chunks means, if one chunk is deemed relevant to the user's question, neighboring chunks are also more likely to be deemed relevant. This means, rather than reducing the number of cuts, chunk overlap increases the odds that neighboring cut pieces are picked together during the retrieval process.\n",
    "\n",
    "However, increasing chunk size and chunk overlap also come with costs. Higher chunk sizes will increase the size of the prompt that is sent to the LLM, increasing the cost of each query. Increasing chunk overlap increases the total number of chunks produced, which can require more memory to store and more computational power to embed. Excessively high chunk overlap can also reduce the breadth of information recieved by the LLM, since higher chunk overlap will make the retrieved documents more likely to be more similar to each other.\n",
    "\n",
    "Chunk size and chunk overlap values must not be excessively low or excessively high. The default values in <a href='https://python.langchain.com/docs/tutorials/rag/#splitting-documents' target='_blank'>LangChain's RAG tutorial</a> (`chunk_size = 1000`, `chunk_overlap = 200`) seem to be reasonable starting points. If you are experiencing difficulty with incomplete answers, increasing chunk overlap (and then chunk size if the problem persists) may be a good place to start. The values you settle at may ultimately depend on your problems, goals, and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da7ed4-9a68-4490-9e36-128062d02442",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "Not specifying that there are _10_ pathways in the myopic model (500 chunk size, 100 chunk overlap) lead to 4 pathways, vs 7 when specified\n",
    "\n",
    "Say that we've observed qualitative pattern is the same but the quantitative numbers may differ because of things like token compression etc.\n",
    "\n",
    "Adding a question mark at the end of 'What are the 10 pathways to 30x30(?)' changes the myopic model's answer from providing 7 pathways to providing 3.\n",
    "\n",
    "When you ask the myopic model how many pathways there are, it will say 10. When you then ask what are the pathways (without saying there are 10), it will tell you the 6 pathways are... (or something similar). This reflects how the LLM doesn't actually digest the whole document, it relies on the retrieval process to provide it with all the relevant information it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d4eaf-c2b3-400c-913f-5fb55ddd68df",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62282f3a-d7de-401a-ab6e-d2cf3076221a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5067f5-dd80-4f2a-9771-6b884145cf19",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style='border: 3px solid #0D335F;' />\n",
    "<hr style='border: 1px solid #5FAE5B;' />\n",
    "\n",
    "# Sources\n",
    "This is a collection of all the links I inserted throughout the doc\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>LangChain RAG tutorial</a>  \n",
    "<a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's Streamlit RAG Demo</a>  \n",
    "<a href='https://www.ibm.com/think/topics/embedding' target='_blank'>What Is Embedding IMB Article</a>  \n",
    "<a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>Recursive Text Splitter Documentation</a>  \n",
    "<a href='https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html' target='_blank'>LangChain ChatPromptTemplate Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b637-5423-417e-ae5a-e7ddc907e245",
   "metadata": {},
   "source": [
    "### Helpful Resources\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/' target='_blank'>LangChain's Tutorials page</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71728bb-f7bf-4428-a564-1e9ed4e2abd0",
   "metadata": {},
   "source": [
    "### Dump:\n",
    "\n",
    "**Breaking mode 1:**  \n",
    "Higher chunk overlap increases the chance that, if one chunk is deemed relevant to the prompt, the chunks surrounding it will also be seen as relevant. In effect, this encourages the RAG model to read more of the context surrounding the chunk where it believes an answer is located. The downside of high chunk overlap is increased computational intensity, since higher overlap means there will be more chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72521836-4573-4c5b-851e-bf66aa9d75fa",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Assume that people have read the full paper, so avoid being over-redundant. Explaining technicalities of code is good. Repeating some stuff from the paper is ok, just avoid being too redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9fff4-52d5-48f4-8c58-5ea6055e66b4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "How should we set up the notebook so users can conveniently enter their OpenAI API key?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b80262-c460-43bd-bf24-d0f7ef20b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
