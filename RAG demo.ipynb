{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32456efd-9fa0-48ec-8954-81c1c1d1cc05",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "</br>\n",
    "The code in this notebook was adapted from langchain's simple <a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>RAG application walkthrough</a> and <a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's streamlit RAG demo</a>.  \n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decc5ef-fc5e-4297-8072-e036de9932e4",
   "metadata": {},
   "source": [
    "Before running this notebook, make sure to open the terminal and run `pip install -r requirements.txt` to load the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b4057-43d0-4390-abe6-9fe05cad22d5",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Setting up RAG\n",
    "\n",
    "This portion of the notebook will walk through the code used to set up our RAG system for the demos.\n",
    "\n",
    "To run all the code in this section and skip to the demo, click the table of contents icon on the left menu bar. Then right click the title of this section, and choose 'Select and Run Cell(s) for this Heading'. Then click the Demos heading to skip to that portion of the notebook. Note that it may take a minute for all the setup cells to finish running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692f18e-8e5f-4672-809a-ca601ecfbdfa",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "    \n",
    "## Initial Setup\n",
    "\n",
    "First, we'll import all the packages we'll need in this notebook. Then we'll set up the chatbot and embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95cd4d35-ffb6-4b4d-a06a-ecc2152f65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecffcc-b1c0-4ff9-8d56-a4a4a17733ef",
   "metadata": {},
   "source": [
    "### Ask for your OpenAI API key if you haven't already set one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c81873c5-0716-4b25-8ec9-fd58f1c132e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    api_key = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "else:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409255bb-a62f-4f14-84a5-a00bc4c10a6d",
   "metadata": {},
   "source": [
    "### Set up the chatbot's language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7edef9a-e026-4c2b-9f3b-6a82a2594afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llama3_llm = ChatOpenAI(model = \"llama3\", api_key = api_key, base_url = \"https://llm.nrp-nautilus.io\",  temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf329b70-6c8d-4fdd-a4fc-760e14b4374c",
   "metadata": {},
   "source": [
    "### Set up the embedding model\n",
    "An embedding model is a machine that, in this case, can take textual data and produce a vector representation (an embedding) of that piece of text. These vector representations allow us to quickly identify sematically similar pieces of text using linear algebra. To read more about embeddings, check out <a href='https://www.ibm.com/think/topics/embedding' target='_blank'>this article from IMB.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bc2684a-4582-47ff-8e72-c82fee4c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "mistral_embeddings = OpenAIEmbeddings(\n",
    "    model = \"embed-mistral\", \n",
    "    api_key = api_key, \n",
    "    base_url = \"https://llm.nrp-nautilus.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aef89-1cfd-49f5-ae99-14cf9ed7595f",
   "metadata": {},
   "source": [
    "### Initial Setup Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8559c0e-5dbe-4372-a993-c15ebdaec666",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Data Processing Pipeline (Indexing)\n",
    "Here's where we start processing the textual data in the document(s) we want our chatbot to use when answering our questions. In our case, this will involve 3 steps: \n",
    "\n",
    "1. Load the document(s)    \n",
    "2. Split the document(s) into smaller pieces  \n",
    "3. Produce vectors representing these smaller pieces, and use those vectors to organize our pieces in a database\n",
    "\n",
    "If we want to change the document(s) our chatbot is using, we'll have to add the new documents and run through this part of the process again (hence the name 'pipeline')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601655d-7436-4008-bd79-d582f5812855",
   "metadata": {},
   "source": [
    "### Load the document(s)\n",
    "This code allows us to load the textual data from PDFs into a format that we can work with. You can also load html files directly from the web by following the steps described in \n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/#loading-documents' target='_blank'>the 'loading documents' portion of the RAG application walkthrough</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "335bfbca-7652-44c2-b550-25bba658abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "def pdf_loader(url):\n",
    "    \"\"\"\n",
    "    Loads the PDF at the given url.\n",
    "\n",
    "    Args:\n",
    "        url (str): the url to the PDF you want to load\n",
    "\n",
    "    Returns: A document containing the text data (and metadata) of the specified PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(url)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3e3b634-4c18-41a8-aced-5f1a38bdc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways_to_30x30_url = 'https://canature.maps.arcgis.com/sharing/rest/content/items/8da9faef231c4e31b651ae6dff95254e/data'\n",
    "docs = pdf_loader(pathways_to_30x30_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972fd63-4d16-4c92-a34b-b58b4aa151b1",
   "metadata": {},
   "source": [
    "To load multiple PDFs: put all the PDFs in a folder, add the PDF folder to the folder containing this jupyter notebook (or any other location were it can be accessed by this jupyter notebook), uncomment the last line of the cell below, write in the path to your folder, and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b44d8aa-9989-4193-afe6-0d6d896488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_pdf_loader(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all PDFs in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): path to the folder containing the all the PDFs you want to load.\n",
    "\n",
    "    Returns: A list of documents, each document representing one PDF\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    return loader.load()\n",
    "\n",
    "#If the PDF folder is in the same folder as this jupyter notebook, the folder path is just the PDF folder's name\n",
    "#Uncomment the line below and paste in the path to your pdf folder to load multiple PDFs.\n",
    "#docs = multiple_pdf_loader('PDF Folder Path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ca3ac-214d-4902-ae21-c543e47a2cad",
   "metadata": {},
   "source": [
    "### Split the document(s) into bite-sized pieces\n",
    "This code will take our document(s) and split their text into smaller sub-sections, sometimes referred to as 'chunks'. There are two important parameters to note in the cell below: `chunk_size` and `chunk_overlap`. \n",
    "\n",
    "The `chunk_size` parameter determines (approximately) how many characters will be in each chunk. The `chunk_overlap` parameter determines how many characters will be shared by any given chunk and the chunk that directly follows it in the text. The importance of `chunk_overlap` is discussed in the Breaking Mode 1 section of this demo.\n",
    "\n",
    "You can read more about langchain's text splitting methods <a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "139d2b2a-d4d3-4d28-b242-7e8248857017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split pdf into 188 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split pdf into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2007b3c-7289-4a7b-8304-0a54c42e646d",
   "metadata": {},
   "source": [
    "### Make an embedding storage system and add the chunks to this storage system\n",
    "First, we'll initialize an embedding storage system, sometimes referred to as a vector store, that will use the embedding model we set up earlier (`embeddings`). Then, when we the add the chunks of our documents to the vector store, it will call the embedding model to create vector representations of those chunks. The vector store will use those vector representations to organize the chunks within its database. This will allow us to  quickly search for relevant pieces of our document(s) later.\n",
    "\n",
    "**TODO:** fact check my description of the under-the-hood activities (I think it's true, but that's just because I don't see how else it could work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "764fa06a-44ee-44f4-b80e-7d5256c66f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f114c9c0-d550-4142-b929-14417710a8c3', '398f3280-d147-48c7-b555-9296f840276b', 'b55791e7-4f01-4f12-adb7-b7dcf5d13b2a']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(mistral_embeddings)\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b77f2-9cb5-4356-81df-52c71886e13b",
   "metadata": {},
   "source": [
    "### Indexing Complete!\n",
    "\n",
    "At this point we've completed the 'indexing' portion of our set up process. This has involved 3 steps:  \n",
    "\n",
    "1. Loading our document(s): We used PyPDFLoader to load our pdf(s) into a format we could process using code.\n",
    "2. Text Splitting: We used a text splitter to break our document(s) into smaller pieces that our LLM will be able to more easily digest.  \n",
    "3. Add chunks to our vector storage system: We used an embedding model to represeent the pieces of our document(s) as vectors. Utilizing the vector embeddings we just made, we organized the pieces of our document(s) in a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58198c46-7394-49bb-a95f-fe52a52d7b05",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Retrieval and Generation\n",
    "\n",
    "Next we will build the infrastructure to find relevant pieces of our document(s) based on our query, and then pass our query along with those relevant pieces of text to the LLM so it can generate an informed response. \n",
    "\n",
    "Retrieving relevant chunks of our document(s) based on our question is often referred to as 'retrieval'.\n",
    "\n",
    "Note: The instructions in the Retrieval and Generation portion of LangChain's RAG demo, <a href='https://python.langchain.com/docs/tutorials/rag/#orchestration' target='_blank'>linked here</a>, uses code more conducive to future modifications and integrations into larger systems. This flexible code is likely preferable when developing a real RAG application, but is more complex than necessary for demonstration purposes. In this demo, we'll take a more 'quick and dirty' approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fbe38-6815-485e-9475-5f324c0dccea",
   "metadata": {},
   "source": [
    "### Make a template for the prompt we'll pass to our LLM\n",
    "\n",
    "With a template we can pass in our question and the relevant context, and we'll get back one complete prompt to pass to our LLM. We'll do this using LangChain's ChatPromptTemplate (<a href='https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html' target='_blank'> documentation linked here</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb66bf59-8435-4e55-b47c-2baf03b30042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Context: [I'll put the context here!]\n",
      "Question: [I'll put the user's question here!]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt_template = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = prompt.invoke(\n",
    "    {\"context\": \"[I'll put the context here!]\", \"input\": \"[I'll put the user's question here!]\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_prompt[0].content)\n",
    "print(example_prompt[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb5676-204e-4a16-9cc2-e363a845394b",
   "metadata": {},
   "source": [
    "### Build a way for the user to query the RAG Chatbot\n",
    "\n",
    "Each time you query a RAG Chatbot, 3 things happen:\n",
    "\n",
    "1. The vector store finds the chunks of the document most relevant to your question (this is the 'retrieval' step).\n",
    "2. Your question and the relevant chunks are bundled into one big prompt.\n",
    "3. That prompt is passed to the LLM, and the LLM uses the relevant chunks to answer your question (this is the 'generation' step).\n",
    "\n",
    "So, we'll build a function that does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c041991d-47b2-4f9c-90a4-d2f4851b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_RAG(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ask our RAG Chatbot a question.\n",
    "\n",
    "    Args: \n",
    "        question (str): the question we want to ask our RAG Chatbot\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary with two keys, 'answer' and 'context'.\n",
    "            'answer' is paired with a string containing the llm's answer to our question\n",
    "            'context' is paired with a string containing the chunks of our document that\n",
    "                were given to the llm to help it answer our question\n",
    "    \"\"\"\n",
    "    relevant_chunks = vector_store.similarity_search(question) \n",
    "    #searches the vector store for chunks of our document semantically similar to the question \n",
    "    #these chunks are returned as LangChain Document objects\n",
    "    \n",
    "    context_str = '\\n\\n'.join(chunk.page_content for chunk in relevant_chunks) \n",
    "    #chunk.page_content gets the chunk's text (since each chunk is a Document object)\n",
    "    #'/n/n'.join(...) builds a string with two new lines between each relevant chunk\n",
    "    \n",
    "    prompt_with_context = prompt.invoke(\n",
    "        {'context': context_str, 'input': question}\n",
    "    )\n",
    "    #builds a prompt using the context string and user's question\n",
    "    \n",
    "    response = llama3_llm.invoke(prompt_with_context)\n",
    "    #gives the prompt to the LLM and gets the LLM's response\n",
    "    \n",
    "    return {'answer': response.content, 'context': context_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a23af72-5ad4-4f52-83a3-0e9c7b7c3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California's 30x30 initiative is a state commitment to conserve 30% of its lands and coastal waters by 2030, as part of an international movement to protect natural areas and combat climate change. The initiative aims to protect and restore biodiversity, expand access to nature, and mitigate and build resilience to climate change. It also aligns with broader state commitments to advance justice, equity, diversity, and inclusion, and sustain economic prosperity.\n"
     ]
    }
   ],
   "source": [
    "pathway_response = ask_RAG(\"What is California's 30x30 initiatve?\")\n",
    "print(pathway_response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be190295-2710-44a5-bcd9-425f6d388769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state policies, practices, and systems, and \n",
      "strategic investments in parks and open \n",
      "spaces, workforce, outdoor programming, \n",
      "and new partnerships.\n",
      "30x30 will conserve lands and coastal waters \n",
      "in ways that will support these two important \n",
      "initiatives and will coordinate closely with leaders \n",
      "of these efforts to ensure investments, policies \n",
      "and programs are mutually supportive and align \n",
      "with one another.  \n",
      "The 30x30 initiative will utilize information \n",
      "and findings from several other state \n",
      "government reports and strategies and advance \n",
      "complementary priorities from these efforts. \n",
      "Other reports and strategies that 30x30 will \n",
      "reference and utilize include:\n",
      "Enhancing Biodiversity \n",
      "2021 California Biodiversity Atlas \n",
      "2020 California Wildlife Barriers \n",
      "2018 California Biodiversity Initiative: A \n",
      "Roadmap for Protecting the State’s Natural \n",
      "Heritage \n",
      "2015 State Wildlife Action Plan \n",
      "Ocean And Coastal Protection \n",
      "Strategic Plan to Protect California’s Coast and \n",
      "Ocean 2020–2025\n",
      "\n",
      "administration priority and elevating the role \n",
      "of nature in the fight against climate change. \n",
      "As part of this Executive Order, California \n",
      "committed to the goal of conserving 30% of our \n",
      "lands and coastal waters by 2030 (30x30). \n",
      "California’s 30x30 initiative is part of an \n",
      "international movement to conserve natural \n",
      "areas across our planet, through which scores \n",
      "of countries have established their own 30x30 \n",
      "commitments. California’s initiative seeks to \n",
      "protect and restore biodiversity, expand access \n",
      "to nature, and mitigate and build resilience \n",
      "to climate change. This effort drives and \n",
      "aligns with broader state commitments to \n",
      "advance justice, equity, diversity, and inclusion, \n",
      "strengthen tribal partnerships, and sustain our \n",
      "economic prosperity, clean energy resources, \n",
      "and food supply.\n",
      "For the purposes of California’s 30x30 goal, an \n",
      "area is considered a “30x30 Conservation Area”  \n",
      "if it meets the following definition: \n",
      "The best available datasets for identifying\n",
      "\n",
      "robust recreation economy, and create jobs in \n",
      "land management and scientific research and \n",
      "monitoring. Maintaining the health of our lands \n",
      "and waters benefits all these economic sectors. \n",
      "California’s long-term prosperity also depends on \n",
      "reducing carbon pollution and shifting to clean \n",
      "energy, building more housing, modernizing our \n",
      "transportation, and maintaining our food supply. \n",
      "Effective planning can and will ensure we conserve \n",
      "the health of our lands and waters and advance \n",
      "30x30 while meeting these other important \n",
      "needs. This planning will be critical, for example, to \n",
      "identify important locations for conservation and \n",
      "those appropriate for other development. \n",
      "The 30x30 initiative will incentivize voluntary, \n",
      "collaborative partnerships to identify and deliver \n",
      "conservation outcomes while supporting the \n",
      "state’s broader economic and climate change \n",
      "priorities. 30x30 will protect our economic \n",
      "prosperity, clean energy resources, and food\n",
      "\n",
      "Governor Newsom issued Executive Order \n",
      "N-82-20 in October 2020 to protect California’s \n",
      "biodiversity and accelerate the use of nature-\n",
      "based solutions in the fight against climate \n",
      "change. The Executive Order commits to \n",
      "conserving 30% of California’s lands and coastal \n",
      "waters by 2030 (30x30).\n",
      "Governor Newsom’s clear call to expand \n",
      "environmental conservation helps lead an \n",
      "international movement to protect our planet. \n",
      "California is part of a growing number of \n",
      "countries—including our own—and other \n",
      "subnational governments who have committed \n",
      "to 30x30. It is an ambitious yet achievable target \n",
      "that, when combined with other conservation \n",
      "measures across our state, will protect our \n",
      "biodiversity and help us mitigate and build \n",
      "resilience to climate change.\n",
      "Approximately 24% of California’s lands and \n",
      "16% of its coastal waters are already conserved \n",
      "based on our definition of 30x30 Conservation \n",
      "Areas. This strategy lays out a vision to conserve\n"
     ]
    }
   ],
   "source": [
    "print(pathway_response['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38c6ae-f783-42b4-a140-6b8bd1a1c7d8",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd799f3-eec6-4445-9ca2-f5abe9fb18ae",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## RAG Builder Set Up\n",
    "\n",
    "This is a helper function for the demos and sandbox. The function takes in various components of a RAG model, and returns an ask_RAG function built using those components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ed3d99a-78fe-4d44-aa42-6220fb8b13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ask_RAG(pdf_path = pathways_to_30x30_url, chunk_size = 1000, chunk_overlap = 200, \n",
    "                  llm = llama3_llm, embedding_model = mistral_embeddings, prompt = prompt):\n",
    "    \"\"\"\n",
    "    Build an ask_RAG function using the provided components\n",
    "\n",
    "    Args: \n",
    "        pdf_path (str): containing either the url of the pdf or\n",
    "            the relative path to the folder containing the pdf(s)\n",
    "            \n",
    "        chunk_size (int): the approximate size of each chunk produced in\n",
    "            the text-splitting process\n",
    "            \n",
    "        chunk_overlap (int): the approximate overlap to between each chunk\n",
    "            and the following chunk produced in the text-splitting process\n",
    "\n",
    "        llm (ChatOpenAI): the LLM used to produce responses to your queries;\n",
    "            an instance of the ChatOpenAI class\n",
    "\n",
    "        embedding_model (OpenAIEmbeddings): the embedding model used to organize\n",
    "            the chunks of your pdf(s) in the vector_store; an instance of the\n",
    "            OpenAIEmbeddings class\n",
    "\n",
    "        prompt (ChatPromptTemplate): the prompt template to use when passing\n",
    "            your question and the relevant context to the llm; an instance of \n",
    "            the ChatPromptTemplate class\n",
    "        \n",
    "    Returns: \n",
    "        function: an ask_RAG function built using the provided components\n",
    "    \"\"\"\n",
    "    #Load the document:\n",
    "    if pdf_path[:4] == 'http':\n",
    "        doc = pdf_loader(pdf_path)\n",
    "    else:\n",
    "        doc = multiple_pdf_loader(pdf_path)\n",
    "\n",
    "    #Split the document into bite-sized pieces\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    splits = text_splitter.split_documents(doc)\n",
    "\n",
    "    #Build a vector store using the provided embedding model\n",
    "    vector_store = InMemoryVectorStore(embedding_model)\n",
    "    #Store the chunks of the document in the vector store\n",
    "    vector_store.add_documents(documents=splits)\n",
    "\n",
    "    #Build an ask_RAG function using vector_store and the provided llm\n",
    "    def new_ask_RAG(question: str):\n",
    "        relevant_chunks = vector_store.similarity_search(question) \n",
    "        context_str = '\\n\\n'.join(chunk.page_content for chunk in relevant_chunks) \n",
    "        prompt_with_context = prompt.invoke(\n",
    "            {'context': context_str, 'input': question}\n",
    "        )\n",
    "        response = llm.invoke(prompt_with_context)\n",
    "        return {'answer': response.content, 'context': context_str}\n",
    "    \n",
    "    return new_ask_RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf7c2-44f1-4abb-b101-2723d6436ebe",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 1: Chunk Cutoffs\n",
    "\n",
    "Because the document is broken into chunks, and the LLM is only provided the chunks that the vector store thinks are most similar to the user's question, chunks neighboring each other in the text may be seperated. One could be deemed relevant and passed to the LLM while the other is deemed not sufficiently relevant and left out. Low chunk_size and chunk_overlap increase the risk of this being a problem, as shown in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3c346-c5aa-43e6-9ff1-88e52304f6b0",
   "metadata": {},
   "source": [
    "### Baseline RAG Agent for Comparison\n",
    "\n",
    "The RAG Agent we built in the set up portion of this notebook split the document into chunks with `chunk_size = 1000` and `chunk_overlap = 200`. These are the values used in <a href='https://python.langchain.com/docs/tutorials/rag/#preview' target='_blank'>LangChain's RAG application walkthrough</a>. \n",
    "\n",
    "To get a baseline response, we'll ask this model to name the 10 pathways to 30x30 that are listed in our document. To answer this question correctly, the retrieval process will have to find chunks of our document that, when taken together, contain all 10 pathways. This shouldn't be too difficult because there are multiple pages in this document with a list of all 10 pathways (namely, pages 4, 5, and 35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6fdfd8c-ad06-41af-abd5-76594e1a680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to achieve 30x30 are: \n",
      "\n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "baseline_results = ask_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(baseline_results['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d7324-92c7-40ae-9c43-0f50af251a04",
   "metadata": {},
   "source": [
    "#### Baseline Results\n",
    "\n",
    "The baseline RAG agent was provided enough context to correctly identify all 10 pathways. To see the context that the retrieval process gave the model, change the value of `show_context_baseline` to `True` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12f4475c-c49f-46ed-abd3-52480236a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_baseline = False #Change this to True to see the context the baselinen LLM was given to answer this question\n",
    "\n",
    "if show_context_baseline:\n",
    "    print(baseline_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0cc5f-996b-4742-a81f-8431e111f671",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Myopic RAG Agent\n",
    "\n",
    "To see how low chunk size and chunk overlap can reduce the effectiveness of the retrieval process, we'll create a RAG Agent based on a document that we split with small `chunk_size` and `chunk_overlap` parameters. In this case, we'll use `chunk_size = 500` and `chunk_overlap = 100`. We'll then ask this model the same question we asked our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a9d3d0a-2b2f-4523-9715-551adfa13559",
   "metadata": {},
   "outputs": [],
   "source": [
    "myo_ask_RAG = build_ask_RAG(chunk_size = 500, chunk_overlap = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51ca7b89-f9f7-4cb6-b41d-c0bfea1e364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not list all 10 pathways to 30x30. However, it mentions three pathways: \n",
      "\n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "\n",
      "Additionally, it mentions two more pathways (10.11 and 10.12) but does not provide the complete list of 10 pathways.\n"
     ]
    }
   ],
   "source": [
    "myo_results = myo_ask_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(myo_results['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38853ae6-bfa6-4367-80c5-3490cd498e8c",
   "metadata": {},
   "source": [
    "#### Myopic Results\n",
    "\n",
    "Clearly the myopic RAG model, with low chunk size and chunk overlap, produced an incomplete answer. Fortunately, the model admitted that it didn't know the remaining 7 pathways. Other models we've tested have not been as forthcoming. Some hallucinated the pathways they couldn't find, as we'll show in breaking mode 2.\n",
    "\n",
    "To see the context this model was given to answer this question, change the value of `show_context_myo` to `True` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a99636ee-e2fd-4b2b-9e51-57b87fb29244",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_myo = False #Change this to True to see the context the myopic LLM was given to answer this question\n",
    "\n",
    "if show_context_myo:\n",
    "    print(myo_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091439dc-ca5b-41c5-8449-4a66e01785ce",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Fixing the Myopic Model; Isolating Chunk Size and Chunk Overlap\n",
    "\n",
    "If your RAG model is producing incomplete answers similar to the answer above, either increasing `chunk_size` or `chunk_overlap` may resolve the issue. Using the myopic model as a baseline, we will see how increasing either parameter can produce a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "753df734-1f72-4b11-978b-954cfbfc5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_chunk_size_RAG = build_ask_RAG(chunk_size = 1000, chunk_overlap = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4ca8429-0baf-4a53-aaa5-e69995c3f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to achieve 30x30 are: \n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "lcs_results = larger_chunk_size_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(lcs_results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7252b9e3-6d93-47ca-a00c-5a3d979235e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_lcs = False\n",
    "\n",
    "if show_context_lcs:\n",
    "    print(lcs_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678ef43-da49-470d-8750-af56e97796e2",
   "metadata": {},
   "source": [
    "Increasing `chunk_size` from `500` to `1000`, while keeping `chunk_overlap` at `100`, was enough to produce an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57688798-c952-47a8-a491-43aef200a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_chunk_overlap_RAG = build_ask_RAG(chunk_size = 500, chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60c5e599-a392-4306-907a-d10bf30c572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to 30x30 are: \n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "lco_results = larger_chunk_overlap_RAG(\"What are the 10 pathways to 30x30?\")\n",
    "print(lco_results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d35633a2-6e2c-40ac-ab6f-ec6d181292ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context_lco = False\n",
    "\n",
    "if show_context_lco:\n",
    "    print(lco_results['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea5dd4-8dab-4bb8-930b-776c1c8379c7",
   "metadata": {},
   "source": [
    "Increasing `chunk_overlap` from `100` to `200`, while leaving the `chunk_size` at `500`, was also enough to produce an accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857c19e-4b2d-4dca-873e-350033d23384",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**TODO:** Rehash some of the interpretation? Maybe just a couple sentence summary of what was said in the paper for a refresher?\n",
    "\n",
    "**Note:** Values of `chunk_size` and `chunk_overlap` at which we observe incorrect answers may change with new versions, but qualitatively the observed patterns are likely to persist.  \n",
    "As new versions of the packages in this demo have released, the values of `chunk_size` and `chunk_overlap` at which the RAG Chatbot produced incorrect answers changed. Typically, when using the more recent versions of these packages, the model was able to accurately answer the prompt for lower values of `chunk_size` and `chunk_overlap`. Increases in the efficiency of tokenization, allowing each chunk to contain more text using fewer tokens, could account for this change. Qualitatively, though, the patterns we observed remained the same. Lower values of `chunk_size` and `chunk_overlap` increased the risk of incomplete answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b7603-e173-4d40-8413-3e2fc8302bd0",
   "metadata": {},
   "source": [
    "### Potential Solutions\n",
    "\n",
    "**Increasing chunk size** forces more neighboring text to be kept together because higher chunk size produces larger units of text. However, these large chunks may still be seperated from their neighbors, and cuts between chunks may still appear in inconvenient places. Increasing chunk size reduces the total number of cuts, but does not address the underlying problem that chunks may be seperated from their neighbors.\n",
    "\n",
    "**Increasing chunk overlap** may be a more comprehensive solution. Higher chunk overlap increases the amount of text shared by two neighboring chunks, thereby increasing their similarity. Higher similarity amongst neighboring chunks means, if one chunk is deemed relevant to the user's question, neighboring chunks are also more likely to be deemed relevant. This means, rather than reducing the number of cuts, chunk overlap increases the odds that neighboring cut pieces are picked together during the retrieval process.\n",
    "\n",
    "However, increasing chunk size and chunk overlap also come with costs. Higher chunk sizes will increase the size of the prompt that is sent to the LLM, increasing the cost of each query. Increasing chunk overlap increases the total number of chunks produced, which can require more memory to store and more computational power to embed. Excessively high chunk overlap can also reduce the breadth of information recieved by the LLM, since higher chunk overlap will make the retrieved documents more likely to be more similar to each other.\n",
    "\n",
    "Chunk size and chunk overlap values must not be excessively low or excessively high. The default values in <a href='https://python.langchain.com/docs/tutorials/rag/#splitting-documents' target='_blank'>LangChain's RAG tutorial</a> (`chunk_size = 1000`, `chunk_overlap = 200`) seem to be reasonable starting points. If you are experiencing difficulty with incomplete answers, increasing chunk overlap (and then chunk size if the problem persists) may be a good place to start. The values you settle at may ultimately depend on your problems, goals, and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da7ed4-9a68-4490-9e36-128062d02442",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "\n",
    "Not specifying that there are _10_ pathways in the myopic model (500 chunk size, 100 chunk overlap) lead to 4 pathways, vs 7 when specified\n",
    "\n",
    "Say that we've observed qualitative pattern is the same but the quantitative numbers may differ because of things like token compression etc.\n",
    "\n",
    "Adding a question mark at the end of 'What are the 10 pathways to 30x30(?)' changes the myopic model's answer from providing 7 pathways to providing 3.\n",
    "\n",
    "When you ask the myopic model how many pathways there are, it will say 10. When you then ask what are the pathways (without saying there are 10), it will tell you the 6 pathways are... (or something similar). This reflects how the LLM doesn't actually digest the whole document, it relies on the retrieval process to provide it with all the relevant information it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d4eaf-c2b3-400c-913f-5fb55ddd68df",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6db6c2-39ed-4600-bb10-e7d766eefd04",
   "metadata": {},
   "source": [
    "### Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d7762-693d-4d9c-b734-41c15d63b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1226a5e8-29f2-4f0c-9b82-b014a9b69767",
   "metadata": {},
   "source": [
    "### Changing Chunk Size and Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dc94e-f812-486b-a2fb-d4f66240d92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f37301a-5afc-4d55-b186-91ffffea295a",
   "metadata": {},
   "source": [
    "### Changing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb4906-7f1c-48a9-a332-8e44e0f393ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfca0e65-60ef-4d70-b443-72213537c45c",
   "metadata": {},
   "source": [
    "### Changing the Embedding Model (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e311d73-aa98-4c80-8b81-a1b7d5c730f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b35f105-4a69-4e44-8c59-9b7651cbc469",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# RAG Sandbox\n",
    "\n",
    "Use this space to experiment with RAG! When testing the effects of adjusting hyperparameters or other components of the RAG chatbot, we recommend using the `build_ask_RAG` function (defined in the RAG Builder Set Up section of this notebook) for convenience. An example using the `build_ask_RAG` function is provided below for demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ee286-1e29-42c1-9b3e-0ece1071cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c781054-0edc-479d-8706-bbea06e77485",
   "metadata": {},
   "source": [
    "**TODO**: Write an example of using build_ask_RAG with a variety of non-default parameters.  \n",
    "**TODO**: Test that providing a relative path to a file of pdfs works in the build_ask_RAG function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5067f5-dd80-4f2a-9771-6b884145cf19",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style='border: 3px solid #0D335F;' />\n",
    "<hr style='border: 1px solid #5FAE5B;' />\n",
    "\n",
    "# Sources\n",
    "This is a collection of all the links I inserted throughout the doc\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>LangChain RAG tutorial</a>  \n",
    "<a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's Streamlit RAG Demo</a>  \n",
    "<a href='https://www.ibm.com/think/topics/embedding' target='_blank'>What Is Embedding IMB Article</a>  \n",
    "<a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>Recursive Text Splitter Documentation</a>  \n",
    "<a href='https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html' target='_blank'>LangChain ChatPromptTemplate Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b637-5423-417e-ae5a-e7ddc907e245",
   "metadata": {},
   "source": [
    "### Helpful Resources\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/' target='_blank'>LangChain's Tutorials page</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71728bb-f7bf-4428-a564-1e9ed4e2abd0",
   "metadata": {},
   "source": [
    "### Dump:\n",
    "\n",
    "**Breaking mode 1:**  \n",
    "Higher chunk overlap increases the chance that, if one chunk is deemed relevant to the prompt, the chunks surrounding it will also be seen as relevant. In effect, this encourages the RAG model to read more of the context surrounding the chunk where it believes an answer is located. The downside of high chunk overlap is increased computational intensity, since higher overlap means there will be more chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72521836-4573-4c5b-851e-bf66aa9d75fa",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Assume that people have read the full paper, so avoid being over-redundant. Explaining technicalities of code is good. Repeating some stuff from the paper is ok, just avoid being too redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9fff4-52d5-48f4-8c58-5ea6055e66b4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "How should we set up the notebook so users can conveniently enter their OpenAI API key?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b80262-c460-43bd-bf24-d0f7ef20b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
