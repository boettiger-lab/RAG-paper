{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32456efd-9fa0-48ec-8954-81c1c1d1cc05",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "</br>\n",
    "The code in this notebook was adapted from langchain's simple <a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>RAG application walkthrough</a> and <a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's streamlit RAG demo</a>.  \n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decc5ef-fc5e-4297-8072-e036de9932e4",
   "metadata": {},
   "source": [
    "Before running this notebook, make sure to open the terminal and run `pip install -r requirements.txt` to load the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b4057-43d0-4390-abe6-9fe05cad22d5",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Setting up RAG\n",
    "\n",
    "This portion of the notebook will walk through the code used to set up our RAG system for the demos.\n",
    "\n",
    "To run all the code in this section and skip to the demo, click the table of contents icon on the left menu bar. Then right click the title of this section, and choose 'Select and Run Cell(s) for this Heading'. Then click the Demos heading to skip to that portion of the notebook. Note that it may take a minute for all the setup cells to finish running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692f18e-8e5f-4672-809a-ca601ecfbdfa",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "    \n",
    "## Initial Setup\n",
    "\n",
    "First we'll set up the chatbot, embedding model, and embedding storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecffcc-b1c0-4ff9-8d56-a4a4a17733ef",
   "metadata": {},
   "source": [
    "### Ask for your OpenAI API key if you haven't already set one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c81873c5-0716-4b25-8ec9-fd58f1c132e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    api_key = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    #TODO: Remove this ^ to avoid messing with their os.environ? Or is it ok?\n",
    "else:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8862d-8110-4129-87f4-ef8d2e7183ea",
   "metadata": {},
   "source": [
    "**TODO:** Replace with some call to a secrets folder for easier use while developing? Make sure this code is safe to distribute (i.e. make sure the fact that me having run the code doesn't mean someone else can come in this notebook and access my API key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409255bb-a62f-4f14-84a5-a00bc4c10a6d",
   "metadata": {},
   "source": [
    "### Set up the chatbot's language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a7edef9a-e026-4c2b-9f3b-6a82a2594afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"llama3\", api_key = api_key, base_url = \"https://llm.nrp-nautilus.io\",  temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf329b70-6c8d-4fdd-a4fc-760e14b4374c",
   "metadata": {},
   "source": [
    "### Set up the embedding model\n",
    "**TODO:** [insert one setence explanation for embedding, and link to further explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6bc2684a-4582-47ff-8e72-c82fee4c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model = \"embed-mistral\", \n",
    "    api_key = api_key, \n",
    "    base_url = \"https://llm.nrp-nautilus.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e4df9-6577-4f8b-9b2b-7b2aa9ff67e0",
   "metadata": {},
   "source": [
    "### Set up the embedding storage system (AKA the vector store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6ef1a93c-8035-42bc-82e0-bdeec2fcb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aef89-1cfd-49f5-ae99-14cf9ed7595f",
   "metadata": {},
   "source": [
    "### Initial Setup Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8559c0e-5dbe-4372-a993-c15ebdaec666",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Data Processing Pipeline (Indexing)\n",
    "Here's where we start processing the textual data in the document(s) we want our chatbot to use when answering our questions. In our case, this will involve 3 steps: \n",
    "\n",
    "1. Load the document(s)    \n",
    "2. Split the document(s) into smaller pieces  \n",
    "3. Produce vectors representing these smaller pieces, and use those vectors to organize our pieces in a database\n",
    "\n",
    "If we want to change the document(s) our chatbot is using, we'll have to add the new documents and run through this part of the process again (hence the name 'pipeline')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601655d-7436-4008-bd79-d582f5812855",
   "metadata": {},
   "source": [
    "### Load the document(s)\n",
    "This code allows us to load the textual data from PDFs into a format that we can work with. You can also load html files directly from the web by following the steps described in \n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/#loading-documents' target='_blank'>the 'loading documents' portion of the RAG application walkthrough</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "335bfbca-7652-44c2-b550-25bba658abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "def pdf_loader(url):\n",
    "    \"\"\"\n",
    "    Loads the PDF at the given url.\n",
    "\n",
    "    Args:\n",
    "        url (str): the url to the PDF you want to load\n",
    "\n",
    "    Returns: A document containing the text data (and metadata) of the specified PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(url)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e3e3b634-4c18-41a8-aced-5f1a38bdc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pdf_loader('https://canature.maps.arcgis.com/sharing/rest/content/items/8da9faef231c4e31b651ae6dff95254e/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972fd63-4d16-4c92-a34b-b58b4aa151b1",
   "metadata": {},
   "source": [
    "To load multiple PDFs: put all the PDFs in a folder, uncomment the last line of the cell below, paste in the path to your folder, and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7b44d8aa-9989-4193-afe6-0d6d896488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_pdf_loader(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all PDFs in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): path to the folder containing the all the PDFs you want to load.\n",
    "\n",
    "    Returns: A list of documents, each document representing one PDF\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    return loader.load()\n",
    "\n",
    "#An example folder file path would look like: 'C:/Users/evan/Downloads/PDF Folder Name'\n",
    "#Uncomment the line below and paste in the path to your pdf folder to load multiple PDFs.\n",
    "#docs = multiple_pdf_loader('paste the path to your folder here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b0c15-243c-4e9d-af79-0d797d1126fc",
   "metadata": {},
   "source": [
    "**TODO:** Test the multiple_pdf_loader function  \n",
    "**TODO:** Also should I remove the docstrings? Do they make the code look scarier than it is just because it makes the cell look much bigger than it should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ca3ac-214d-4902-ae21-c543e47a2cad",
   "metadata": {},
   "source": [
    "### Split the document(s) into bite-sized pieces\n",
    "This code will take our document(s) and split their text into smaller sub-sections, sometimes referred to as 'chunks'. There are two important parameters to note in the cell below: `chunk_size` and `chunk_overlap`. \n",
    "\n",
    "The `chunk_size` parameter determines (approximately) how many characters will be in each chunk. The `chunk_overlap` parameter determines how many characters will be shared by any given chunk and the chunk that directly follows it in the text. The importance of `chunk_overlap` is discussed in the article (see breaking mode 1), and will be demonstrated later in this notebook.\n",
    "\n",
    "You can read more about langchain's text splitting methods <a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "139d2b2a-d4d3-4d28-b242-7e8248857017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split pdf into 171 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split pdf into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2007b3c-7289-4a7b-8304-0a54c42e646d",
   "metadata": {},
   "source": [
    "### Add the pieces to the embedding storage system (AKA the vector store)\n",
    "Under the hood, this code is actually doing two things. When we set up the vector store earlier, we told it which embedding model to use. Now, when we the add the chunks of our documents to the vector store, it first will call the embedding model to create vector representations of those chunks. Then it will use those vector representations to organize the chunks within the database. This will allow us to  quickly search for relevant pieces of our document(s) later.\n",
    "\n",
    "**TODO:** fact check my description of the under-the-hood activities (I think it's true, but that's just because I don't see how else it could work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "764fa06a-44ee-44f4-b80e-7d5256c66f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1b94b0aa-c33c-401c-a0c1-50de1201af4a', '27f2dafa-fc85-444e-82f9-75e8245c7b09', '88e50023-93ee-4fd9-9a42-6d807bbef6a0']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "assert len(all_splits) == len(document_ids), \"len(all_splits) does not match len(document_ids): The vector store likely wasn't reset before adding the new splits\"\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b77f2-9cb5-4356-81df-52c71886e13b",
   "metadata": {},
   "source": [
    "### Indexing Complete!\n",
    "\n",
    "At this point we've completed the 'indexing' portion of our set up process. This has involved 3 steps:  \n",
    "\n",
    "1. **Loading our document(s)**: We used PyPDFLoader to load our pdf(s) into a format we could process using code.\n",
    "2. **Text Splitting**: We used a text splitter to break our document(s) into smaller pieces that our chatbot will be able to more easily digest.  \n",
    "3. **Add chunks to our vector storage system**: We used an embedding model to represeent the pieces of our document(s) as vectors. Utilizing the vector embeddings we just made, we organized the pieces of our document(s) in a database.\n",
    "                                                                                                                                                                                                        \n",
    "Next we will set up a 'retriever' which will use this organized database to retrieve relevant pieces of our document(s) based on the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58198c46-7394-49bb-a95f-fe52a52d7b05",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Retrieval and Generation\n",
    "\n",
    "**TODO** insert Description of what this section builds\n",
    "\n",
    "Note: The instructions in the Retrieval and Generation portion of LangChain's RAG demo, <a href='https://python.langchain.com/docs/tutorials/rag/#orchestration' target='_blank'>linked here</a>, uses code more conducive to future modifications and integrations into larger systems. This flexible code is likely preferable when developing a real RAG application, but is more complex than necessary for demonstration purposes. In this demo, we'll take a more 'quick and dirty' approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c1a19-18bd-48b6-818b-f9f8cfb2ac0b",
   "metadata": {},
   "source": [
    "### Build the retriever\n",
    "\n",
    "We'll build an agent that will take in a user's question, search our vector store for semantically similar chunks, and then return those passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "21cd3c04-40db-4e52-b512-f05ac056d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fbe38-6815-485e-9475-5f324c0dccea",
   "metadata": {},
   "source": [
    "### Make a template for the prompt we'll pass to our chatbot\n",
    "\n",
    "We'll do this using LangChain's ChatPromptTemplate (<a href='https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html' target='_blank'> documentation linked here</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eb66bf59-8435-4e55-b47c-2baf03b30042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Context: [I'll put the context here!]\n",
      "Question: [I'll put the user's question here!]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt_template = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = prompt.invoke(\n",
    "    {\"context\": \"[I'll put the context here!]\", \"input\": \"[I'll put the user's question here!]\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_prompt[0].content)\n",
    "print(example_prompt[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb5676-204e-4a16-9cc2-e363a845394b",
   "metadata": {},
   "source": [
    "### Build a way for the user to query the RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c041991d-47b2-4f9c-90a4-d2f4851b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code correctly answered the question, without any changes to the chunk size or parameters ###\n",
    "### It wouldn't give me a way to see which parts of the original document were referenced, however ###\n",
    "### I decided to pivot to code closer to Professor Boettiger's, so we can inspect the retriever's actions ###\n",
    "\n",
    "# def ask_RAG(question: str):\n",
    "#     relevant_chunks = vector_store.similarity_search(question) \n",
    "#     #searches the vector store for semantically related chunks \n",
    "#     #these chunks are returned as LangChain Document objects\n",
    "    \n",
    "#     context_str = '\\n\\n'.join(chunk.page_content for chunk in relevant_chunks) \n",
    "#     #chunk.page_content gets the chunk's text, since each chunk is a Document object\n",
    "#     #'/n/n'.join(...) builds a string with two new lines between each relevant chunk\n",
    "    \n",
    "#     prompt_with_context = prompt.invoke(\n",
    "#         {'context': context_str, 'question': question}\n",
    "#     )\n",
    "#     #build a prompt to pass to the chatbot using the context string and user's question\n",
    "\n",
    "#     return llm.invoke(prompt_with_context).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6375d2a4-3160-4c87-bfe5-5d95441dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847fded-5d3b-46b2-877b-153ab117a07b",
   "metadata": {},
   "source": [
    "### NOTES!!!\n",
    "\n",
    "Re-running the document splitter without clearing the vector store can produce misleading results (the changes to the document splits won't be reflected).\n",
    "\n",
    "**Gave correct answer:** chunk size: 1000, chunk overlap: 200, 100, 50  \n",
    "**Gave half answer:** chunk size: 500, chunk overlap: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d27fc3d0-41de-49fe-a445-a3443bb23289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 pathways to achieve 30x30 are: \n",
      "\n",
      "1. Accelerate Regionally Led Conservation\n",
      "2. Execute Strategic Land Acquisitions\n",
      "3. Increase Voluntary Conservation Easements\n",
      "4. Enhance Conservation of Existing Public Lands and Coastal Waters\n",
      "5. Institutionalize Advance Mitigation\n",
      "6. Expand and Accelerate Environmental Restoration and Stewardship\n",
      "7. Strengthen Coordination Among Governments\n",
      "8. Align Investments to Maximize Conservation Benefits\n",
      "9. Advance and Promote Complementary Conservation Measures\n",
      "10. Evaluate Conservation Outcomes and Adaptively Manage\n"
     ]
    }
   ],
   "source": [
    "results = rag_chain.invoke({'input': 'What are the 10 pathways to 30x30?'})\n",
    "print(results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b62017cf-5671-4fff-a989-820dcd811323",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_context = False\n",
    "# Change show_context to True if you want to see the passages used to produce this answer\n",
    "\n",
    "if show_context:\n",
    "    for context in results['context']:\n",
    "        print(context.metadata, '\\n')\n",
    "        print(context.page_content, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38c6ae-f783-42b4-a140-6b8bd1a1c7d8",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style=\"border: 5px solid #0D335F;\" />\n",
    "<hr style=\"border: 2px solid #5FAE5B;\" />\n",
    "\n",
    "# Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf7c2-44f1-4abb-b101-2723d6436ebe",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d3d0a-2b2f-4523-9715-551adfa13559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6d4eaf-c2b3-400c-913f-5fb55ddd68df",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #5FAE5B;\" />\n",
    "\n",
    "## Breaking mode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62282f3a-d7de-401a-ab6e-d2cf3076221a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5067f5-dd80-4f2a-9771-6b884145cf19",
   "metadata": {},
   "source": [
    "</br>\n",
    "<hr style='border: 3px solid #0D335F;' />\n",
    "<hr style='border: 1px solid #5FAE5B;' />\n",
    "\n",
    "# Sources\n",
    "This is a collection of all the links I inserted throughout the doc\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/rag/' target='_blank'>LangChain RAG tutorial</a>  \n",
    "<a href='https://huggingface.co/spaces/cboettig/streamlit-demo/blob/main/pages/rag.py' target='_blank'>Professor Boettiger's Streamlit RAG Demo</a>  \n",
    "<a href='https://python.langchain.com/docs/how_to/recursive_text_splitter/' target='_blank'>Recursive Text Splitter Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b637-5423-417e-ae5a-e7ddc907e245",
   "metadata": {},
   "source": [
    "### Helpful Resources\n",
    "\n",
    "<a href='https://python.langchain.com/docs/tutorials/' target='_blank'>LangChain's Tutorials page</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71728bb-f7bf-4428-a564-1e9ed4e2abd0",
   "metadata": {},
   "source": [
    "### Dump:\n",
    "\n",
    "**Breaking mode 1:**  \n",
    "Higher chunk overlap increases the chance that, if one chunk is deemed relevant to the prompt, the chunks surrounding it will also be seen as relevant. In effect, this encourages the RAG model to read more of the context surrounding the chunk where it believes an answer is located. The downside of high chunk overlap is increased computational intensity, since higher overlap means there will be more chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72521836-4573-4c5b-851e-bf66aa9d75fa",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Assume that people have read the full paper, so avoid being over-redundant. Explaining technicalities of code is good. Repeating some stuff from the paper is ok, just avoid being too redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9fff4-52d5-48f4-8c58-5ea6055e66b4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "How should we set up the notebook so users can conveniently enter their OpenAI API key?\n",
    "\n",
    "What do we think about the blue and green horizontal lines? Are there tweaks we could make that would be better?\n",
    "\n",
    "I assume the actual notebook shouldn't have the %pip install cells right?  \n",
    "And how do I add the -U in requirements.txt (or I guess just the U since I assume the q just means don't fill the screen with text)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b80262-c460-43bd-bf24-d0f7ef20b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
